import requests
import csv
import time

def fetch_data(url, headers, params):
    """æŠ“å–ç¶²é è³‡æ–™"""
    try:
        res = requests.get(url, headers=headers, params=params)
        res.raise_for_status()  # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼
        return res.json()
    except requests.exceptions.RequestException as e:
        print(f"âŒ è«‹æ±‚éŒ¯èª¤ï¼š{e}")
        return None

def parse_data(data):
    """è§£æ JSON è³‡æ–™ï¼Œæå–éœ€è¦çš„è·ç¼ºè³‡è¨Š"""
    jobs = []
    try:
        if data and data['data'] and data['data']['list']:  # ç¢ºä¿è³‡æ–™å­˜åœ¨
            for job in data['data']['list']:
                job_name = job.get('jobName', '')
                company = job.get('custName', '')
                address = job.get('jobAddrNoDesc', '')
                link = f"https://www.104.com.tw/job/{job.get('jobNo', '')}"  # ä½¿ç”¨ .get()
                
                jobs.append([job_name, company, address, link])
        else:
            print("âš ï¸  æ²’æœ‰æ‰¾åˆ°è·ç¼ºè³‡æ–™")
    except KeyError as e:
        print(f"âŒ  JSON æ ¼å¼éŒ¯èª¤ï¼š{e}")
    return jobs

def save_to_csv(filename, data):
    """å°‡è·ç¼ºè³‡è¨Šå¯«å…¥ CSV æª”æ¡ˆ"""
    with open(filename, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["è·ç¼ºåç¨±", "å…¬å¸åç¨±", "åœ°å€", "ç¶²å€"])
        writer.writerows(data)
    print(f"ğŸ“„ å·²æˆåŠŸå¯«å…¥ {filename}")

def main():
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Referer": "https://www.104.com.tw/jobs/search/",
        "Accept": "application/json, text/plain, */*",
        "Accept-Encoding": "gzip, deflate, br",
        "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8"
    }

    url = "https://www.104.com.tw/jobs/search/list"
    params = {
        "ro": "0",
        "kwop": "1",
        "keyword": "Python",
        "order": "11",
        "page": "1",
        "mode": "s",
        "jobsource": "2018indexpoc"
    }

    all_jobs = []
    page = 1
    while True:  # è¿´åœˆæŠ“å–å¤šé è³‡æ–™
        params['page'] = page
        data = fetch_data(url, headers, params)
        if not data:
            break

        jobs = parse_data(data)
        if not jobs:
            break

        all_jobs.extend(jobs)

        # å¢åŠ å»¶é²ï¼Œé¿å…éæ–¼é »ç¹çš„è«‹æ±‚
        time.sleep(1)  # 1 ç§’å»¶é²
        page += 1

        # ç°¡å–®çš„åˆ†é åœæ­¢æ¢ä»¶ (æŠ“å–å‰ 5 é ) - å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´
        if page > 5:
            print("ğŸ›‘ å·²é”åˆ°æœ€å¤§é æ•¸ï¼Œåœæ­¢æŠ“å–")
            break

    print(f"âœ… ç¸½å…±æŠ“åˆ° {len(all_jobs)} ç­†è·ç¼ºè³‡æ–™")
    save_to_csv("static.csv", all_jobs)

if __name__ == "__main__":
    main()

